{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's currently sunny in Boston with a high of 22 degrees Celsius and a low of 10 degrees Celsius. \", additional_kwargs={}, response_metadata={}, id='run-31c874b7-89f2-4329-a664-f9a7fd2e573c-0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "model = OllamaFunctions(model=\"llama3.2:1b\", format=\"json\")\n",
    "\n",
    "model = model.bind_tools(\n",
    "    tools=[\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, \" \"e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    function_call={\"name\": \"get_current_weather\"},\n",
    ")\n",
    "\n",
    "model.invoke(\"what is the weather in Boston?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='{\"result\": 4569896}' additional_kwargs={} response_metadata={} id='run-471d9061-0c2b-42f5-a222-808516be8035-0'\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.agents import Tool\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "\n",
    "def sumar_numeros(a: int, b: int) -> int:\n",
    "    \"\"\"Suma dos números y devuelve el resultado.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "class SumaInput(BaseModel):\n",
    "    \"\"\"Define los parámetros que recibirá la función sumar_numeros.\"\"\"\n",
    "    a: int = Field(description=\"El primer número.\")\n",
    "    b: int = Field(description=\"El segundo número.\")\n",
    "\n",
    "# Definir la función en el formato que espera OllamaFunctions\n",
    "suma_function = {\n",
    "    \"name\": \"sumar_numeros\",\n",
    "    \"description\": \"Suma dos números y devuelve el resultado.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"a\": {\"type\": \"integer\", \"description\": \"El primer número\"},\n",
    "            \"b\": {\"type\": \"integer\", \"description\": \"El segundo número\"}\n",
    "        },\n",
    "        \"required\": [\"a\", \"b\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Crear el modelo\n",
    "model = OllamaFunctions(\n",
    "    model=\"llama3.2:1b\",\n",
    "    format=\"json\"\n",
    ")\n",
    "\n",
    "# Vincular la función\n",
    "model = model.bind_tools(\n",
    "    tools=[suma_function],\n",
    "    function_call={\"name\": \"sumar_numeros\"}\n",
    ")\n",
    "\n",
    "# Realizar la invocación\n",
    "response = model.invoke(\"cuanto es 23444 mas 2828282?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Realizar la invocación\n",
    "model.invoke(\"suma es 23444 mas 2828282?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# List of URLs to load documents from\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "# Load documents from the URLs\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter with specified chunk size and overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "# Split the documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "# Create embeddings for documents and store them in a vector store\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding = OllamaEmbeddings(\n",
    "    model=\"llama3.2:1b\",\n",
    "),\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "# Initialize the LLM with Llama 3.1 model\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain combining the prompt template and LLM\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG application class\n",
    "class RAGApplication:\n",
    "    def __init__(self, retriever, rag_chain):\n",
    "        self.retriever = retriever\n",
    "        self.rag_chain = rag_chain\n",
    "    def run(self, question):\n",
    "        # Retrieve relevant documents\n",
    "        documents = self.retriever.invoke(question)\n",
    "        # Extract content from retrieved documents\n",
    "        doc_texts = \"\\\\n\".join([doc.page_content for doc in documents])\n",
    "        # Get the answer from the language model\n",
    "        answer = self.rag_chain.invoke({\"question\": question, \"documents\": doc_texts})\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RAG application\n",
    "rag_application = RAGApplication(retriever, rag_chain)\n",
    "# Example usage\n",
    "question = \"What is prompt engineering\"\n",
    "answer = rag_application.run(question)\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
